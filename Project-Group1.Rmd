---
title: "Group1 Project"
author: "Zeynep Bal, Ayşegül Karakuş, Burcu Oral"
date: "03 01 2020"
output: html_document
---
## INTRODUCTION

Data mining approaches are widely used in the prediction of the result of sports games. Since it is possible to win money by guessing which team will win or the total number of goals scored in football, people are looking for ways to predict the outcome acurately. In football predictions it is important to determine the features correctly so that the model results in accurate predictions. There are several publications that tries to predict the outcomes with many different features.

## RELATED LITERATURE 

The most common methods used in football outcome predictions are Bayesian classifiers/regressors, Logistic Regression(LogR) , Support Vector Machines (SVM), Random Forest (RF), Gradient Boosting (GB) and K-nearest Neighbors.Besides them Deep learning and  Artificial Neural Networks (ANN) are also employed. Since Poisson Regression, LogR,GB and RF are employed in this project, the literature about these methods are given below. 
Igiri et al. [1], used both artificial neural network (ANN) and logistic regression (LogR) techniques with 85% and 93% prediction accuracy  respectively.
With Bayesian approaches References [2] and [3] achieved around 92% accuracy. 

For example [4] used SVM,LogR and RF for prediction with optimized parameters: for SVM, linear kernel and C= 0.005 with the prediction accuracy of 63.25; for LogR, C=0.001 with accuracy of 63.3; and for RF, 80 trees and 2 min. split with the accuracy of 62.26.

Ulmer and Fernandez achieved error rates of .48 with linear classifier; .50 with Random Forest and .50 with SVM [5].They used a small feature dataset.

It is also possible to have time dependent model like the work of Rue et al.[6], a Bayesian linear model is used to predict outcome taking into account the relative strength of attack and defense of each team.

In [7], the author used three major models: Generalized Boosted Models (GBM), K-nearest neighbor and Naive Bayes classification. Using GBM, they attained 60.22% accuracy on average, while the other models were not as accurate. 

Regarding the poisson nature of football game results can result in good accuracy with smaller feature set [8].While the Poisson models predict the number of goals conceded and scored, the other statistical models restrict their prediction to match result. However, when goal driven models and match results driven model were compared in [9], it was found that LogR and Poisson approaches yield almost similar prediction performance.

Features that are used in predictions are divirsified; the playing conditions (home or away), fatigue levels, team selected by the manager and many other factors[10].There are team based and individual player based approaches for feature engineering. There can be ‘match-related’ and ‘external’ features. Match related features are actual events within the sport’s match. For example,meters gained, passes made can be regarded as match-related.External features can be recent form, travel, players available for the match, etc.[11].External features are known before the match but only an average of match related features for a certain number of past matches for these teams is known. Buursma [12] found that using an average across the past 20 matches for match-related features resulted in the best classification accuracy.

Leung et al. [13] analyze a set of teams that are the most similar to each of the competing teams, find the results of the games between the teams in each of the two sets, and use those game results to predict the outcome of the game between the original two teams. This approach analyzes past game results and a number of statistics about each of the teams from each of those games; such as passing attempts, rushing attempts, and turnovers for and against. Statistical data are stored in two data structures: (a) a list storing every game played over a given time and (b) a list storing all teams with their corresponding statistics from the season. So the approach parses the team lists and creates a map with every point representing a team. The distance between two points on the map is proportional to their similarity.

Reference [2] divided the data set into two;(a) non-physiological factors: weather, history of five previous matches, results against/for team, home game and players’ psychological state and (b) physiological factors: average age of the players, the number of injured main players, average match in a week, performance of main players, the performance of all players and average number of goals for all home and away matches.

Paper [14] used points for and against, win-loss record, home and away performance, performance in previous 4 games, ranking, location and player availability for premier league predictions and [15] used goals for, for shots,shots on goals, corner kicks, free kicks, ball possesion and fouls data for prediction in World Cup and reference [16] used goals for, goals against, result of previous match, top scorers, day since previous match as features in Dutch Eredivisie prediction.

In addition to all of these possible features there are some index calculation methods such as ELO rating and ICT index etc.. These indexes take into account basic knowledge, such as wins and losses, strength of schedule or point differential, but also knowledge of previous seasons [17] 


## APPROACH 

In this project GB, RF, LogR and poisson regression are used. The statistic of teams in each game, red card numbers, total goals conceded and scored against each other, total win, lost and draw numbers against each other and team names(id) are used as predictors for GB, RF and LogR. Two main approaches are tried: 1)classification of match result (home win,away win,draw) 2)prediction of away and home team scores and calculating the results (home win,away win,draw) afterwards. Afterwards, it is determined that the first option is more suitable to our project purpose. When the values were missing for a particular match, they were filled with average value. Cross validation was used to optimize the parameters of models. For poisson regression reference [18] is used as template.  

```{r, echo=FALSE, warning=FALSE,message=FALSE, results='hide'}
library(data.table)
library(dplyr)
library(lubridate)
library(caret)
library(gbm)
library(glmnet)
library(rattle)
library(tidyr)
library(ModelMetrics)
library(randomForest)
library(e1071)
library(rattle)

matches = read.table('matches.csv', header=TRUE, sep=',')
matches$match_hometeam_name=ifelse(matches$match_hometeam_name=="Manchester United","Manchester Utd",as.character(matches$match_hometeam_name))
matches$match_awayteam_name=ifelse(matches$match_awayteam_name=="Manchester United","Manchester Utd",as.character(matches$match_awayteam_name))
matches$match_hometeam_name=ifelse(matches$match_hometeam_name=="Aston Villa (Eng)","Aston Villa",as.character(matches$match_hometeam_name))
matches$match_awayteam_name=ifelse(matches$match_awayteam_name=="Aston Villa (Eng)","Aston Villa",as.character(matches$match_awayteam_name))
matches$match_hometeam_name=ifelse(matches$match_hometeam_name=="Newcastle Utd","Newcastle",as.character(matches$match_hometeam_name))
matches$match_awayteam_name=ifelse(matches$match_awayteam_name=="Newcastle Utd","Newcastle",as.character(matches$match_awayteam_name))
matches$match_hometeam_name=ifelse(matches$match_hometeam_name=="West Ham (Eng)","West Ham",as.character(matches$match_hometeam_name))
matches$match_awayteam_name=ifelse(matches$match_awayteam_name=="West Ham (Eng)","West Ham",as.character(matches$match_awayteam_name))

epl=filter(matches, league_id=="148")
epl=mutate(epl,epoch=as_datetime(epl$epoch))
final_data=epl[,c(2,1,4,5,3,7,8,9,10)]

finish=filter(final_data,match_status=='Finished')
#setting lose,draw,win as 1 0 
for(i in 1:length(finish$match_hometeam_score)){
  if(finish$match_hometeam_score[i]>finish$match_awayteam_score[i]){  
    finish$win[i]=1
    finish$lose[i]=0
    finish$draw[i]=0
  }
  if(finish$match_awayteam_score[i]>finish$match_hometeam_score[i]){  
    finish$lose[i]=1
    finish$win[i]=0
    finish$draw[i]=0
  }
  if(finish$match_hometeam_score[i]==finish$match_awayteam_score[i]){ 
    finish$draw[i]=1
    finish$win[i]=0
    finish$lose[i]=0
  }
}

# USING STATS DATA
stats = read.table('stats.csv', header=TRUE, sep=',')
stats$home_BallPossession=gsub( "%", "", as.character(stats$home_BallPossession))
stats$away_BallPossession=gsub( "%", "", as.character(stats$away_BallPossession))
stats_used=stats[,c(1,2,5,6,11,14,15,20,21)]

stats_used$ home_BallPossession=as.numeric(stats_used$ home_BallPossession)
stats_used$ away_BallPossession=as.numeric(stats_used$ away_BallPossession)

#Merging stats data with matches
sts=finish
stcol=ncol(sts)+1
for (i in 1:nrow(sts)){  
  for (j in 1: nrow(stats_used)){   
    if (sts$match_id[i]==stats_used$match_id[j]){
      sts[i,stcol]=stats_used[j,2] # or k, k+3 from 2 to 9
      sts[i,stcol+1]=stats_used[j,3]
      sts[i,stcol+2]=stats_used[j,4]
      sts[i,stcol+3]=stats_used[j,5]
      sts[i,stcol+4]=stats_used[j,6]
      sts[i,stcol+5]=stats_used[j,7]
      sts[i,stcol+6]=stats_used[j,8]
      sts[i,stcol+7]=stats_used[j,9]
    } 
  } 
} 

colnames_stats=colnames(stats_used)
colnames(sts)[13:20]<-c(colnames_stats[2],colnames_stats[3],colnames_stats[4],colnames_stats[5],colnames_stats[6],colnames_stats[7],colnames_stats[8],colnames_stats[9])

# USING BOOKING DATA 

book = read.table('booking.csv', header=TRUE, sep=',')
reds=book%>% filter(card=='red card')

home_Red=data.frame(reds[,c(1,3,4)],stringsAsFactors = FALSE)
home_Red$home_fault=as.character(home_Red$home_fault)
home_Red$card=as.integer(home_Red$card)
home_Red$home_fault[home_Red$home_fault==""] <- "NA"
home_Red=filter(home_Red,home_Red$home_fault!="NA")
reds_home<- home_Red%>%group_by(match_id)%>%summarize(reds_home=sum(card))

away_Red=reds[,c(1,5,4)]
away_Red$away_fault=as.character(away_Red$away_fault)
away_Red$card=as.integer(away_Red$card)
away_Red$away_fault[away_Red$away_fault==""] <- "NA"
away_Red=filter(away_Red,away_Red$away_fault!="NA")
reds_away<- away_Red%>%group_by(match_id)%>%summarize(reds_away=sum(card))

#Merging booking data with matches
new=sts
new$red_away=rep(0,nrow(new))
for (i in 1:nrow(new)){  
  for (j in 1: nrow(reds_away)){   
    if (new$match_id[i]==reds_away$match_id[j]){new$red_away[i]=reds_away$reds_away[j]
    }}}


new$red_home=rep(0,nrow(new))
for (i in 1:nrow(new)){  
  for (j in 1: nrow(reds_home)){   
    if (new$match_id[i]==reds_home$match_id[j]){new$red_home[i]=reds_home$reds_home[j]
    }}}

data=new[,-c(3,4)]


#Giving data its last shape combining historical data for each team, till here everything was match based

data<-data%>%group_by(match_hometeam_id,match_awayteam_id)%>%summarise(home_goal=sum(match_hometeam_score),away_goal=sum(match_awayteam_score),
                                                                       no_win=sum(win),no_lose=sum(lose),no_draw=sum(draw),
                                                                       home_possession=mean(home_BallPossession),away_possession=mean(away_BallPossession),
                                                                       home_keeper=sum(home_GoalkeeperSaves),away_keeper=sum(away_GoalkeeperSaves),
                                                                       home_attempt=sum(home_GoalAttempts),away_attempt=sum(away_GoalAttempts),
                                                                       home_block=sum(home_BlockedShots),away_block=sum(away_BlockedShots),
                                                                       red_home=sum(red_home),red_away=sum(red_away))



merg=final_data[,c(5,1,2,8,9)]

col=ncol(merg)+1
for (i in 1:nrow(merg)){  
  for (j in 1: nrow(data)){   
    if (merg$match_hometeam_id[i]==data$match_hometeam_id[j] && merg$match_awayteam_id[i]==data$match_awayteam_id[j] ){
      merg[i,col]=data[j,3] # or k, k+3 from 2 to 9
      merg[i,col+1]=data[j,4]
      merg[i,col+2]=data[j,5]
      merg[i,col+3]=data[j,6]
      merg[i,col+4]=data[j,7]
      merg[i,col+5]=data[j,8]
      merg[i,col+6]=data[j,9]
      merg[i,col+7]=data[j,10]
      merg[i,col+8]=data[j,11]
      merg[i,col+9]=data[j,12]
      merg[i,col+10]=data[j,13]
      merg[i,col+11]=data[j,14]
      merg[i,col+12]=data[j,15]
      merg[i,col+13]=data[j,16]
      merg[i,col+14]=data[j,17]
      
    } 
  } }

dat_y=merg[,c(1,4,5)]
dat_x=merg[,-c(4,5)]

y<- as.data.frame(lapply(dat_y, function(x) ifelse(is.na(x), 'empty', x)))
x<- as.data.frame(lapply(dat_x, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x)))
data_All<-merge(x,y,by="match_id")

data_al=filter(data_All,data_All$match_hometeam_score!='empty')
result=ifelse(as.numeric(data_al[,19])>as.numeric(data_al[,20]),yes='home',no=ifelse(as.numeric(data_al[,19])<as.numeric(data_al[,20]),yes='away',no='draw'))
data_all=cbind(data_al[,1:18],result)

#RPS CALCULATION FUNCTIONS 

RPS_matrix<- function(probs,outcomes){
  probs=as.matrix(probs)
  outcomes=as.matrix(outcomes)
  probs=t(apply(t(probs), 2, cumsum))
  outcomes=t(apply(t(outcomes), 2, cumsum))
  RPS = apply((probs-outcomes)^2,1,sum) / (ncol(probs)-1)
  return(RPS)
}

rps_calc=function(data,colbegin)
{
  outcomes=matrix(0,nrow(data),3)
  for(i in 1:nrow(data))
  {
    if(test$result[i]=='home')
    {
      outcomes[i,1]=1
    }
    if(test$result[i]=='draw')
    {
      outcomes[i,2]=1
    }
    if(test$result[i]=='away')
    {
      outcomes[i,3]=1
    }
  }
  RPS_TEST=RPS_matrix(data,outcomes)
  return(RPS_TEST)
}




set.seed(123)

smp_siz = floor(0.8*nrow(data_all))

train_ind = sample(seq_len(nrow(data_all)),size = smp_siz)
train =data_all[train_ind,]
test=data_all[-train_ind,]

# GB FITTING
fit_gbm<-train(train[,-c(1,19)],train[,19],method='gbm',trControl=trainControl(method = 'cv'))

fit_gbm$bestTune

estimate_gbm=predict(fit_gbm,test[,-c(1,19)],n.trees = 50)

gbm_prob=predict(fit_gbm,test[,-c(1,19)],n.trees = 50,type="prob")

prob_gbm=cbind(prob=gbm_prob,real=test[,19])

#RESULT OF PREDICTIONS AND REAL 
View(prob_gbm)

#RPS calculation for GBM

probs_gbm=cbind(home=as.numeric(gbm_prob[,1]),draw=as.numeric(gbm_prob[,2]),away=as.numeric(gbm_prob[,3]))
RPS_gbm=rps_calc(probs_gbm,3)
mean(RPS_gbm)

#For Accuracy
compare_gbm=cbind(estimate_gbm,test[,19])
COUNT=0
for (i in 1:nrow(compare_gbm)){if (compare_gbm[i,1]!=compare_gbm[i,2]){
  COUNT=COUNT +1
  COUNT
}}
error_gbm=COUNT/nrow(compare_gbm)

# RF FITTING
fit_rf<-train(train[,-c(1,19)],train[,19],method='rf',trControl=trainControl(method = 'cv'))

fit_rf$bestTune

estimate_rf=predict(fit_rf,test[,-c(1,19)],mtry=2)
rf_prob=predict(fit_rf,test[,-c(1,19)],mytry=2,type="prob")

prob_rf=cbind(prob=rf_prob,real=test[,19])
View(prob_rf)

#RPS calculation for RF

probs_rf=cbind(home=as.numeric(rf_prob[,1]),draw=as.numeric(rf_prob[,2]),away=as.numeric(rf_prob[,3]))
RPS_rf=rps_calc(probs_rf,3)
mean(RPS_rf)

#For Accuracy

compare_rf=cbind(estimate_rf,test[,19])
COUNT=0
for (i in 1:nrow(compare_rf)){if (compare_rf[i,1]!=compare_rf[i,2]){
  COUNT=COUNT +1
  COUNT
}}
error_rf=COUNT/nrow(compare_rf)

# LogR FITTING
x_train=train[,-c(1,19)]
y_train=train[,c(19)]
x_test=test[,-c(1,19)]
y_test=test[,c(19)]

x_train_scale<-scale(x_train, center = TRUE, scale = TRUE)
x_test_scale<-scale(x_test, center = TRUE, scale = TRUE)

fit_logR <- cv.glmnet(x_train_scale,y_train, alpha = 1,type.measure="class",family="multinomial")


estimate_logR=predict(fit_logR,x_test_scale,s=fit_logR$lambda.1se,type="class")

logR_prob=predict(fit_logR,x_test_scale,s=fit_logR$lambda.1se,type="response")

prob_logR=cbind(prob=logR_prob[, ,1],real=as.character(y_test))
View(prob_logR)

#RPS calculation for LogR
probs_logR=cbind(home=as.numeric(logR_prob[,1,]),draw=as.numeric(logR_prob[,2,]),away=as.numeric(logR_prob[,3,]))
RPS_LogR=rps_calc(probs_logR,3)
mean(RPS_LogR)

#For Accuracy
compare_logR=cbind(estimate_logR,as.character(y_test))
COUNT=0
for (i in 1:nrow(compare_logR)){if (compare_logR[i,1]!=compare_logR[i,2]){
  COUNT=COUNT +1
  COUNT
}}
error_logR=COUNT/nrow(compare_logR)
error_logR

## POISSON REGRESSION 

train_pos =data_al[train_ind,]
test_pos=data_al[-train_ind,]

poisson_model <- rbind(data.frame(goals=train_pos$match_hometeam_score,team=train_pos$match_hometeam_id, opponent=train_pos$match_awayteam_id,home=1),
                       data.frame(goals=train_pos$match_awayteam_score,team=train_pos$match_awayteam_id, opponent=train_pos$match_hometeam_id,home=0)) 

poisson_model$goals<-as.numeric(poisson_model$goals)
poisson_model$team<-as.factor(poisson_model$team)
poisson_model$opponent<-as.factor(poisson_model$opponent)
poisson_reg=poisson_model%>% glm(goals ~ home + team +opponent, family=poisson(link=log),data=.)

result_match <- function(model, home, away, max_goals=10){
  home_goals<- predict(model,data.frame(home=1, team=home, opponent=away), type="response")
  away_goals <- predict(model,data.frame(home=0, team=away,opponent=home), type="response")
  
  dpois(0:max_goals, home_goals) %o% dpois(0:max_goals, away_goals) 
}


home=0
draw=0
away=0
for(i in 1:nrow(test)){
  match <- result_match(poisson_reg,  as.factor(test_pos[i,2]),as.factor(test_pos[i,3]), max_goals=10)
  # home win
  home[[i]]=sum(match[lower.tri(match)])
  # draw
  draw[[i]]=sum(diag(match))
  # away win
  away[[i]]=sum(match[upper.tri(match)])
}

prob_poisson=matrix(0,nrow(test),3)
prob_poisson=cbind(home,draw,away,real=as.character(y_test))
View(prob_poisson)

# PRS Calculation for poisson
probs_pos=prob_poisson[,c(1,2,3)]
probs_pos=as.matrix(probs_pos)
probs_pos=cbind(home=as.numeric(probs_pos[,1]),draw=as.numeric(probs_pos[,2]),away=as.numeric(probs_pos[,3]))
RPS_poisson=rps_calc(probs_pos,3)
mean(RPS_poisson)


#Accuracy for Poisson
result_poisson=ifelse(as.numeric(prob_poisson[,1])>as.numeric(prob_poisson[,3]), yes='home', no=ifelse(as.numeric(prob_poisson[,1]<prob_poisson[,3]),yes='away',no='draw'))
compare_poisson=cbind(result_poisson,real=as.character(y_test))

COUNT=0
for (i in 1:nrow(compare_poisson)){if (compare_poisson[i,1]!=compare_poisson[i,2]){
  COUNT=COUNT +1
  COUNT
}}
error_poisson=COUNT/nrow(compare_poisson)

error_poisson

###########SUMMARY OF THE RESULTS

table<-array(numeric(),c(4,3))
table<-data.frame(table)
colnames(table)[1] <- "Summary"
colnames(table)[2] <- "RPS"
names(table)[3] <- "Error"

table[1,]<-c('GBM',mean(RPS_gbm),error_gbm)
table[2,]<-c('RF',mean(RPS_rf),error_rf)
table[3,]<-c('LogR',mean(RPS_LogR),error_logR)
table[4,]<-c('Poisson',mean(RPS_poisson),error_poisson)
table[,2]<-as.numeric(table[,2])
table[,3]<-as.numeric(table[,3])
table[,2]<-format(table[,2], digits=2)
table[,3]<-format(table[,3], digits=2)

```
 sd

## Gradiendt Boosting (GB)
Gradient Boosted Trees are used for training. In this approach, the number of trees are determined as 50, interaction dept as 1, shrinkage as 0.1 and number of observations in nodes are 10 with GBM. The probabilities of the home, draw and away conditions for the first 6 columns are as follow:

```{r, echo=FALSE, warning=FALSE,message=FALSE}
prob_gbm[,1]<-as.numeric(prob_gbm[,1])
prob_gbm[,2]<-as.numeric(prob_gbm[,2])
prob_gbm[,3]<-as.numeric(prob_gbm[,3])
prob_gbm[,1]<-format(prob_gbm[,1], digits=2)
prob_gbm[,2]<-format(prob_gbm[,2], digits=2)
prob_gbm[,3]<-format(prob_gbm[,3], digits=2)

head(prob_gbm)

```


## Random Forest(RF)
This is the  tree based method used in the project. In this method, there are classification and regression trees trained with 2/3 percent of the whole training data.Trees are grown until minimal number of observations per tree leaf obtained. According to our models 2 trees are grown and probabilities of match results are calculated accordingly.

```{r, echo=FALSE, warning=FALSE,message=FALSE}
prob_rf[,1]<-as.numeric(prob_rf[,1])
prob_rf[,2]<-as.numeric(prob_rf[,2])
prob_rf[,3]<-as.numeric(prob_rf[,3])
prob_rf[,1]<-format(prob_rf[,1], digits=2)
prob_rf[,2]<-format(prob_rf[,2], digits=2)
prob_rf[,3]<-format(prob_rf[,3], digits=2)

head(prob_rf)
```


## Logistic Regression (LogR)
Lasso logistic regression is also used to calculate the probabilites. Lasso provides good prediction accuracy due to the shrinkage of the coefficients. It reduces overfitting by eliminating irrelevant variables.

```{r, echo=FALSE, warning=FALSE,message=FALSE}

head(prob_logR)
```

## SELECTED APPROACH: POISSON DISTRUBUTION

The predicted probabilities for home/draw/away conditions are made with poisson distributions. As it is stated in the related literature section, poisson distribution can result in good accuracy with minimum number of variables. In our project,the only variables used for poission distributions are home id, away id, home number of goals and away number of goals. The number of goals scored by both home and away are poisson distributed.

```{r, echo=FALSE, warning=FALSE,message=FALSE}

par(mfrow=c(1,2))
hist(final_data$match_hometeam_score,right=F, breaks=8, xlim=c(0,8), xlab="Home Goals", ylab="Number of Games",main="Home Score(goals)", col="red")
hist(final_data$match_awayteam_score, right=F,  breaks=7, xlim=c(0,7), xlab="Away Goals", ylab="Number of Games",main="Away Score(goals)", col="blue")

```

When the histograms are compared, it is seen that the number of goals scored by the home teams have higher frequencies for the higher number of the goals whereas away team have lower number of goals. However, in all the leagues there are so called the strong teams which scores higher goals no matter which one is hosting the game. Hence these teams have generally high probability, thus lower bet scores, to win. 

```{r, echo=FALSE, warning=FALSE,message=FALSE}
team<-unique(epl$match_hometeam_id)
team_id<-unique(epl$match_hometeam_name)
teams<-cbind(team, team_id)
teams<-as.data.frame(teams)
###Match Analysis-Home Performances
finish_epl=filter(epl,match_status=='Finished')
home_agg<- aggregate(finish_epl[,9:10],by=list(finish_epl$match_awayteam_id),FUN=mean)
away_agg<- aggregate(finish_epl[,9:10],by=list(finish_epl$match_hometeam_id),FUN=mean)
avg_team<-merge(home_agg, teams, by.x = "Group.1", by.y = "team", all = TRUE)
avg_team<-avg_team[,c(2,3,4)]
avg_team[order(avg_team$match_hometeam_score),]
```
As can be seen from the table, Manchester City, Liverpool and Wolves are the top scorer teams in English Premier Leage. Hence, they have higher chances to score in a game and thus has higher chances to win. Because of this reason, the predicted match games are always in favor of the teams which have higher statistics. 

To sum up, team id of home and away teams and the number of goals scored by each team are taken as the features in this approach. For the probability calculations using Poisson regression, a matrix is obtained by simulation with using historical data. This matrix shows that the probabilities of two teams scoring a specific number of goals( selected max 10 goals). It calculates the odds of draw by summing diagonal, the odds of home team win by summing lower triangle and the odds of away team win by summing upper triangle.

```{r, echo=FALSE, warning=FALSE,message=FALSE}
prob_poisson[,1]<-as.numeric(prob_poisson[,1])
prob_poisson[,2]<-as.numeric(prob_poisson[,2])
prob_poisson[,3]<-as.numeric(prob_poisson[,3])
prob_poisson[,1]<-format(prob_poisson[,1], digits=2)
prob_poisson[,2]<-format(prob_poisson[,2], digits=2)
prob_poisson[,3]<-format(prob_poisson[,3], digits=2)
head(prob_poisson)
```

## Discussion of the results
The results obtained with this four models are compared in this section with the table given below. 

```{r, echo=FALSE, warning=FALSE,message=FALSE}
table
```

When the errors of classifying the match results as home/draw/away, the errors are ranked as LogR<GBM<RF<Poisson. The highest error is obtained with poisson regression. Actually, this is expected due to the structure of the Poisson model. Because poission regression predicts only home and away conditions, it fails for the games resulted in draw. 

On the other hand, the RPS values are ranked as Poisson<LogR<RF<GBM. The minimum rps values are obtained with Poission regression model. As stated in related work section, the RPS values of logR is close to poission regression. Hence, when determining the preferred model, the best candidates were these two methods. Nevertheless, the performance metric RPS is suggested because it is expressed as probability distributions and takes into account how close the distribution is to the observed value.

## CONCLUSIONS AND FEATURE WORKS
Poisson Regression is a very successful model in this case. The propabilty of match results are calculated with this model. Also the probabilites are calculated with other models which are GB, RF and LogR and the results are compared with rps performance metric and classification error. When the performance metrics were taken into consideration, it can be said that Poisson Regression was the best model because of it gets smaller value in terms of rps value. In addition, poisson regression is a good choice because the number of goals fit to Poisson distributions. As a conclusion,  the best model is poisson regression for that prediction. Hence, the probability of home-away or draw of the teams were calculated based on poisson regression and related estimates were made on this model.

As for feature work, suitable features can be added to Poission distribution dataset. The features of our dataset are just numerical values but there are other factors that may effect the match result. The motivation of the team, the absence of a footboller, the tiredness and travelling amount may also be predictive. Otherwise, this model will give results in favor of strong team. Plus, the code can be altered to predict draw cases as well. In other word, the poisson distribution can be rewritten for multivariate analysis. 


##CODE
The code can be found at [https://github.com/BU-IE-582/fall19-aysegulkrkus/blob/master/HW_4_Group1.R](https://github.com/BU-IE-582/fall19-aysegulkrkus/blob/master/HW_4_Group1.R).


### REFERENCES 
[1] R.Igiri, C. Peace, A.Nwachukwu and E.Okechukwu, IOSR journal of Engineering, Vol.4, Issue 12,2014

[2] Baio G. and Blangiardo M., Bayesian Hierarchical Modelling for the Prediction of Football Results, Seminari del Dipartimento di Statistica,2009

[3] Owramipur F.,Eskandarian P., and Mozneb F.S., Football Result Prediction with Bayesian Network in Spanish League-Barcelona Team, International Journal of Computer Theory and Engineering, Vol. 5, No. 5, 2013

[4] Bosch, P., Predicting the winner of NFL-games using Machine and Deep Learning. 

[5]Ulmer B.and Fernandez M., Predicting Soccer Match Results In The English Premier League, Stanford, 2014

[6]Rue H. And  Salvesen O., Prediction And Retrospective Analysis Of Soccer Matches In A League. Journal Of The Royal Statistical Society: Series D, 2000

[7]Hijmans A., Dutch football prediction using machine learning classifiers 

[8]Saraiva E. F.,  Suzuki A.K., Filho C. A. O., Louzada F., Predicting football scores via Poisson regression model: applications to the National Football League,Communications for Statistical Applications and Methods  Vol. 23, No. 4, 297–319,2016

[9]Constantinaou C., Fenton N.E. and Neil M., A Bayesian network model for forecasting Association Football match outcomes, Working Papers, Queen Mary University, 2012

[10]Sathe S., Kasat D., Kulkarni N.,  Satao R.,Predictive Analysis Of Premier League Using Machine Learning, International Journal of Innovative Research in Computer and Communication Engineering Vol. 5, Issue 3, March 2017

[11] Bunker R.P.and Thabtah F., A machine learning framework for sport result prediction, Applied Computing and Informatics, Volume 15, Issue 1, 2019, Pages 27-33

[12] Buursma D., Predicting sports events from past results “Towards effective betting on football matches”, in: Conference Paper, presented at 14th Twente Student Conference on IT, Twente, Holland, 21 January 2011, 2001.

[13] Carson K. Leung, Kyle W. Joseph, Sports data mining: predicting results for the college football games, 18th International Conference on Knowledge-Based and Intelligent Information & Engineering Systems, Procedia Computer Science 35, 710 – 719,2014 

[14]Mccabe, A. and Trevathan, J.,Artificial Intelligence in Sports Prediction. Fifth International Conference on Information Technology: New Generations, 2008

[15]Huang, K, and Chang, W., A neural network method for prediction of 2006 World Cup Football Ge. The 2010 International Joint Conference on Neural Networks, 2010

[16]Tax, N. and Joustra, Y.P., Predicting the Dutch football competition using public data: A machine learning approach, Trans. Knowl. Data Eng. Vol 10 , 2015

[17]N. Silver, "fivethirtyeight.com," ESPN, September 2014. [Online]. Available: https://fivethirtyeight.com/features/introducing-nfl-elo-ratings/.  

[18]Predicting football results with Poisson regression, March 2013 from : https://opisthokonta.net/?p=276 
